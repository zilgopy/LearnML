{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization(shape):\n",
    "    n_in = shape[0]\n",
    "    stddev = np.sqrt(2.0 / n_in)\n",
    "    return np.random.normal(0, stddev, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Build sum layer ($y = \\Sigma wx + b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum():\n",
    "    def __init__(self,neuron_no,back_neuron_no): \n",
    "        #imagine a 4 -> 3 network , back is 4 , mine is 3 \n",
    "        # W: [4,3]\n",
    "        self.W = he_initialization((back_neuron_no,neuron_no))\n",
    "        # B: [1,3]\n",
    "        self.B = he_initialization((neuron_no,))\n",
    "        # GW: [4,3]\n",
    "        # GB: [1,3]\n",
    "        # GD: [1,4]\n",
    "        self.lr \n",
    "    \n",
    "    \n",
    "    def forward(self,X): \n",
    "        # X : [1,4] , W : [4,3] -> [1,3] + [1,3] => [1,3]\n",
    "        self.out = np.dot(X,self.W) + self.B\n",
    "        self.X = X\n",
    "        return self.out\n",
    "    # frontGD = [1,3]\n",
    "    def back(self,frontGD):\n",
    "        # X: [1,4], X.T : [4,1]  dot -> [4,3]\n",
    "        self.GW =  np.dot(self.X.T,frontGD)/frontGD.shape[0]\n",
    "        self.GB = np.sum(frontGD,axis=0)/frontGD.shape[0]\n",
    "        self.GD = np.dot(frontGD,self.W.T)\n",
    "        self.W = self.W - self.lr * self.GW\n",
    "        self.B = self.B - self.lr * self.GB\n",
    "        return self.GD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build ReLU Activation Layer ($y=max(x,0)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self): \n",
    "        pass\n",
    "    \n",
    "    def forward(self,X): \n",
    "        self.out = np.maximum(X,0)\n",
    "        return self.out\n",
    "    # frontGD = [1,3]\n",
    "    def back(self,frontGD):\n",
    "        self.GD = frontGD.copy()\n",
    "        self.GD[self.out<=0]=0\n",
    "        return self.GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Build SoftMax Activation Layer ($y=\\frac {e^{x_i}} {\\Sigma e^x}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax():\n",
    "    def __init__(self): \n",
    "        pass\n",
    "    \n",
    "    def forward(self,X): \n",
    "\n",
    "        e_x = np.exp(X - np.max(X,axis=1,keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1,keepdims=True)\n",
    "\n",
    "    def back(self,frontGD):\n",
    "        self.GD = frontGD.copy()\n",
    "        return self.GD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build categorical_crossentropy loss function ($y=-\\Sigma ylog\\hat y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss():\n",
    "    def __init__(self): \n",
    "        self.loss= 0\n",
    "        self.predict=0.0\n",
    "        \n",
    "    def calcLoss(self,y,y_hat): \n",
    "        self.y=y\n",
    "        self.y_hat=y_hat\n",
    "        loss = -np.log(y_hat+1e-10)*y\n",
    "        self.loss =self.loss+ np.sum(loss)\n",
    "        self.predict += np.sum(np.argmax(y_hat,axis=1)== np.argmax(y,axis=1))\n",
    "    def back(self):\n",
    "        return self.y_hat-self.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Define Dense layer, combination of sum + ReLU activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self,neuron_no,back_neuron_no): \n",
    "        self.sum = Sum(neuron_no,back_neuron_no)\n",
    "        self.activate = ReLU()\n",
    "        \n",
    "    def forward(self,X): \n",
    "        \n",
    "        return self.activate.forward(self.sum.forward(X))\n",
    "\n",
    "    # frontGD = [1,3]\n",
    "    def back(self,frontGD):\n",
    "        return self.sum.back(self.activate.back(frontGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Define Output layer, combination of sum + SoftMax activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output():\n",
    "    def __init__(self,neuron_no,back_neuron_no): \n",
    "        self.sum = Sum(neuron_no,back_neuron_no)\n",
    "        self.activate = SoftMax()\n",
    "    def forward(self,X): \n",
    "        return self.activate.forward(self.sum.forward(X))\n",
    "\n",
    "    def back(self,frontGD):\n",
    "        return self.sum.back(self.activate.back(frontGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Load the fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainx, trainy), (testx,testy)=keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.reshape(trainx,(trainx.shape[0],-1))\n",
    "test_x = np.reshape(testx,(testx.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x/255\n",
    "test_x=test_x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = keras.utils.to_categorical(trainy)\n",
    "test_y = keras.utils.to_categorical(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Sequential Neuron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functionChain(funcs,value):\n",
    "    if not funcs:\n",
    "        return value\n",
    "    first_func = funcs[0]\n",
    "    rest_funcs = funcs[1:]\n",
    "\n",
    "    return functionChain(rest_funcs, first_func(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN():\n",
    "    def __init__(self): \n",
    "        self.layers = []\n",
    "        self.forward = []\n",
    "        self.back = []\n",
    "\n",
    "    def addLayer(self,layer,neuron_no,input_shape=None):\n",
    "        if len(self.layers) == 0 :\n",
    "            self.layers.append(layer(neuron_no,input_shape))\n",
    "        else :\n",
    "            self.layers.append(layer(neuron_no,self.backnn_no))\n",
    "        self.backnn_no = neuron_no\n",
    "    def compile(self,loss=Loss,lr=0.03):\n",
    "        self.loss=loss\n",
    "        for i in self.layers:\n",
    "            i.sum.lr = lr\n",
    "            self.forward.append(i.forward)\n",
    "        for i in self.layers[::-1]:\n",
    "            self.back.append(i.back)\n",
    "    def summary(self):\n",
    "        print(\"Model summary:\")\n",
    "        print(\"-\"*30)\n",
    "        print(\"Model Name\\tOutput\\tParameters\")\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            print(f\"{layer.__class__.__name__}{i+1}\\t[batch_size,{layer.sum.B.size }]\\t{layer.sum.B.size+layer.sum.W.size}\")\n",
    "\n",
    "    def fit(self,trainx,trainy,validx,validy,epochs=10,batchsize=1,):\n",
    "\n",
    "        for epoch in range (0,epochs):\n",
    "            #Train Part\n",
    "            loss=self.loss()\n",
    "            i=0\n",
    "            while i<trainx.shape[0]:\n",
    "                batchindex = min(i+batchsize,trainx.shape[0])\n",
    "\n",
    "                loss.calcLoss(trainy[i:batchindex],functionChain(self.forward,trainx[i:batchindex]))\n",
    "                functionChain(self.back,loss.back())               \n",
    "                i=batchindex\n",
    "                \n",
    "            print(\"epoch\",epoch+1,\" accuracy score:\", loss.predict/trainx.shape[0],\"avg loss:\",loss.loss/trainx.shape[0])\n",
    "            #Validation Part\n",
    "            loss=self.loss()\n",
    "            j=0\n",
    "            while j<validx.shape[0]:\n",
    "                batchindex = min(j+batchsize,validx.shape[0])\n",
    "                loss.calcLoss(validy[j:batchindex],functionChain(self.forward,validx[j:batchindex]))       \n",
    "                j=batchindex\n",
    "            print(\"epoch\",epoch+1,\" validation accuracy score:\", loss.predict/validx.shape[0],\"avg validation loss:\",loss.loss/validx.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "------------------------------\n",
      "Model Name\tOutput\tParameters\n",
      "Dense1\t[batch_size,64]\t50240\n",
      "Dense2\t[batch_size,32]\t2080\n",
      "Output3\t[batch_size,10]\t330\n"
     ]
    }
   ],
   "source": [
    "model=SNN()\n",
    "model.addLayer(Dense,64,input_shape=train_x.shape[1])\n",
    "model.addLayer(Dense,32)\n",
    "model.addLayer(Output,10)\n",
    "model.compile(lr=0.01)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  accuracy score: 0.6521833333333333 avg loss: 1.0917654923484579\n",
      "epoch 1  validation accuracy score: 0.7468 avg validation loss: 0.7382853879390856\n",
      "epoch 2  accuracy score: 0.77955 avg loss: 0.6428361518721978\n",
      "epoch 2  validation accuracy score: 0.7886 avg validation loss: 0.6066771250651872\n",
      "epoch 3  accuracy score: 0.8058333333333333 avg loss: 0.556529021019899\n",
      "epoch 3  validation accuracy score: 0.8089 avg validation loss: 0.5511329112197152\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatchsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[218], line 36\u001b[0m, in \u001b[0;36mSNN.fit\u001b[0;34m(self, trainx, trainy, validx, validy, epochs, batchsize)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i\u001b[38;5;241m<\u001b[39mtrainx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m     34\u001b[0m     batchindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(i\u001b[38;5;241m+\u001b[39mbatchsize,trainx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 36\u001b[0m     loss\u001b[38;5;241m.\u001b[39mcalcLoss(trainy[i:batchindex],\u001b[43mfunctionChain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatchindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m     functionChain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mback,loss\u001b[38;5;241m.\u001b[39mback())               \n\u001b[1;32m     38\u001b[0m     i\u001b[38;5;241m=\u001b[39mbatchindex\n",
      "Cell \u001b[0;32mIn[217], line 7\u001b[0m, in \u001b[0;36mfunctionChain\u001b[0;34m(funcs, value)\u001b[0m\n\u001b[1;32m      4\u001b[0m first_func \u001b[38;5;241m=\u001b[39m funcs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m rest_funcs \u001b[38;5;241m=\u001b[39m funcs[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m functionChain(rest_funcs, \u001b[43mfirst_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[206], line 8\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X): \n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivate\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(trainx=train_x,trainy=train_y,validx=test_x,validy=test_y,epochs=20,batchsize=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
