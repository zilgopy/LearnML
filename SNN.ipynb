{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization(shape):\n",
    "    n_in = shape[0]\n",
    "    stddev = np.sqrt(2.0 / n_in)\n",
    "    return np.random.normal(0, stddev, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Build sum layer ($y = \\Sigma wx + b$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum():\n",
    "    def __init__(self,neuron_no,back_neuron_no): \n",
    "        #imagine a 4 -> 3 network , back is 4 , mine is 3 \n",
    "        # W: [4,3]\n",
    "        self.W = he_initialization((back_neuron_no,neuron_no))\n",
    "        # B: [1,3]\n",
    "        self.B = he_initialization((neuron_no,))\n",
    "        # GW: [4,3]\n",
    "        # GB: [1,3]\n",
    "        # GD: [1,4]\n",
    "        self.lr =0\n",
    "    \n",
    "    \n",
    "    def forward(self,X): \n",
    "        # X : [1,4] , W : [4,3] -> [1,3] + [1,3] => [1,3]\n",
    "        self.out = np.dot(X,self.W) + self.B\n",
    "        self.X = X\n",
    "        return self.out\n",
    "    # frontGD = [1,3]\n",
    "    def back(self,frontGD):\n",
    "        # X: [1,4], X.T : [4,1]  dot -> [4,3]\n",
    "        self.GW =  np.dot(self.X.T,frontGD)/frontGD.shape[0]\n",
    "        self.GB = np.sum(frontGD,axis=0)/frontGD.shape[0]\n",
    "        self.GD = np.dot(frontGD,self.W.T)\n",
    "        self.W = self.W - self.lr * self.GW\n",
    "        self.B = self.B - self.lr * self.GB\n",
    "        return self.GD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Build ReLU Activation Layer ($y=max(x,0)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    def __init__(self): \n",
    "        pass\n",
    "    \n",
    "    def forward(self,X): \n",
    "        self.out = np.maximum(X,0)\n",
    "        return self.out\n",
    "    # frontGD = [1,3]\n",
    "    def back(self,frontGD):\n",
    "        self.GD = frontGD.copy()\n",
    "        self.GD[self.out<=0]=0\n",
    "        return self.GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Build SoftMax Activation Layer ($y=\\frac {e^{x_i}} {\\Sigma e^x}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax():\n",
    "    def __init__(self): \n",
    "        pass\n",
    "    \n",
    "    def forward(self,X): \n",
    "\n",
    "        e_x = np.exp(X - np.max(X,axis=1,keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1,keepdims=True)\n",
    "\n",
    "    def back(self,frontGD):\n",
    "        self.GD = frontGD.copy()\n",
    "        return self.GD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build categorical_crossentropy loss function ($L=-\\Sigma ylog\\hat y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss():\n",
    "    def __init__(self): \n",
    "        self.loss= 0\n",
    "        self.predict=0.0\n",
    "        \n",
    "    def calcLoss(self,y,y_hat): \n",
    "        self.y=y\n",
    "        self.y_hat=y_hat\n",
    "        loss = -np.log(y_hat+1e-10)*y\n",
    "        self.loss =self.loss+ np.sum(loss)\n",
    "        self.predict += np.sum(np.argmax(y_hat,axis=1)== np.argmax(y,axis=1))\n",
    "    def back(self):\n",
    "        return self.y_hat-self.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Define Dense layer, combination of sum + ReLU activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self,neuron_no,back_neuron_no): \n",
    "        self.sum = Sum(neuron_no,back_neuron_no)\n",
    "        self.activate = ReLU()\n",
    "        \n",
    "    def forward(self,X): \n",
    "        \n",
    "        return self.activate.forward(self.sum.forward(X))\n",
    "\n",
    "    # frontGD = [1,3]\n",
    "    def back(self,frontGD):\n",
    "        return self.sum.back(self.activate.back(frontGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Define Output layer, combination of sum + SoftMax activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Output():\n",
    "    def __init__(self,neuron_no,back_neuron_no): \n",
    "        self.sum = Sum(neuron_no,back_neuron_no)\n",
    "        self.activate = SoftMax()\n",
    "    def forward(self,X): \n",
    "        return self.activate.forward(self.sum.forward(X))\n",
    "\n",
    "    def back(self,frontGD):\n",
    "        return self.sum.back(self.activate.back(frontGD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Load the fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainx, trainy), (testx,testy)=keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.reshape(trainx,(trainx.shape[0],-1))\n",
    "test_x = np.reshape(testx,(testx.shape[0],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=train_x/255\n",
    "test_x=test_x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = keras.utils.to_categorical(trainy)\n",
    "test_y = keras.utils.to_categorical(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Sequential Neuron Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def functionChain(funcs,value):\n",
    "    if not funcs:\n",
    "        return value\n",
    "    first_func = funcs[0]\n",
    "    rest_funcs = funcs[1:]\n",
    "\n",
    "    return functionChain(rest_funcs, first_func(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN():\n",
    "    def __init__(self): \n",
    "        self.layers = []\n",
    "        self.forward = []\n",
    "        self.back = []\n",
    "\n",
    "    def addLayer(self,layer,neuron_no,input_shape=None):\n",
    "        if len(self.layers) == 0 :\n",
    "            self.layers.append(layer(neuron_no,input_shape))\n",
    "        else :\n",
    "            self.layers.append(layer(neuron_no,self.backnn_no))\n",
    "        self.backnn_no = neuron_no\n",
    "    def compile(self,loss=Loss,lr=0.03):\n",
    "        self.loss=loss\n",
    "        for i in self.layers:\n",
    "            i.sum.lr = lr\n",
    "            self.forward.append(i.forward)\n",
    "        for i in self.layers[::-1]:\n",
    "            self.back.append(i.back)\n",
    "    def summary(self):\n",
    "        print(\"Model summary:\")\n",
    "        print(\"-\"*30)\n",
    "        print(\"Model Name\\tOutput\\tParameters\")\n",
    "        for i,layer in enumerate(self.layers):\n",
    "            print(f\"{layer.__class__.__name__}{i+1}\\t[batch_size,{layer.sum.B.size }]\\t{layer.sum.B.size+layer.sum.W.size}\")\n",
    "\n",
    "    def fit(self,trainx,trainy,validx,validy,epochs=10,batchsize=1,):\n",
    "\n",
    "        for epoch in range (0,epochs):\n",
    "            #Train Part\n",
    "            loss=self.loss()\n",
    "            i=0\n",
    "            while i<trainx.shape[0]:\n",
    "                batchindex = min(i+batchsize,trainx.shape[0])\n",
    "\n",
    "                loss.calcLoss(trainy[i:batchindex],functionChain(self.forward,trainx[i:batchindex]))\n",
    "                functionChain(self.back,loss.back())               \n",
    "                i=batchindex\n",
    "                \n",
    "            print(\"epoch\",epoch+1,\" accuracy score:\", loss.predict/trainx.shape[0],\"avg loss:\",loss.loss/trainx.shape[0])\n",
    "            #Validation Part\n",
    "            loss=self.loss()\n",
    "            j=0\n",
    "            while j<validx.shape[0]:\n",
    "                batchindex = min(j+batchsize,validx.shape[0])\n",
    "                loss.calcLoss(validy[j:batchindex],functionChain(self.forward,validx[j:batchindex]))       \n",
    "                j=batchindex\n",
    "            print(\"epoch\",epoch+1,\" validation accuracy score:\", loss.predict/validx.shape[0],\"avg validation loss:\",loss.loss/validx.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape,train_y.shape,test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary:\n",
      "------------------------------\n",
      "Model Name\tOutput\tParameters\n",
      "Dense1\t[batch_size,64]\t50240\n",
      "Dense2\t[batch_size,32]\t2080\n",
      "Output3\t[batch_size,10]\t330\n"
     ]
    }
   ],
   "source": [
    "model=SNN()\n",
    "model.addLayer(Dense,64,input_shape=train_x.shape[1])\n",
    "model.addLayer(Dense,32)\n",
    "model.addLayer(Output,10)\n",
    "model.compile(lr=0.01)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1  accuracy score: 0.6609833333333334 avg loss: 1.04312955112509\n",
      "epoch 1  validation accuracy score: 0.7604 avg validation loss: 0.6991382556763156\n",
      "epoch 2  accuracy score: 0.7944333333333333 avg loss: 0.6094322444007864\n",
      "epoch 2  validation accuracy score: 0.8014 avg validation loss: 0.5800350857170178\n",
      "epoch 3  accuracy score: 0.81695 avg loss: 0.5354093145884482\n",
      "epoch 3  validation accuracy score: 0.8148 avg validation loss: 0.5338825049965471\n",
      "epoch 4  accuracy score: 0.827 avg loss: 0.4992525171266931\n",
      "epoch 4  validation accuracy score: 0.8221 avg validation loss: 0.5074980282316104\n",
      "epoch 5  accuracy score: 0.8347333333333333 avg loss: 0.4760962289784653\n",
      "epoch 5  validation accuracy score: 0.8283 avg validation loss: 0.4897905691644074\n",
      "epoch 6  accuracy score: 0.8405666666666667 avg loss: 0.4593734365712465\n",
      "epoch 6  validation accuracy score: 0.8331 avg validation loss: 0.47620466252452953\n",
      "epoch 7  accuracy score: 0.8453833333333334 avg loss: 0.4457583707238765\n",
      "epoch 7  validation accuracy score: 0.837 avg validation loss: 0.4650208799982729\n",
      "epoch 8  accuracy score: 0.8491333333333333 avg loss: 0.4345888054524934\n",
      "epoch 8  validation accuracy score: 0.8405 avg validation loss: 0.45582761006806716\n",
      "epoch 9  accuracy score: 0.8519333333333333 avg loss: 0.4250337251479063\n",
      "epoch 9  validation accuracy score: 0.8408 avg validation loss: 0.44843893498838805\n",
      "epoch 10  accuracy score: 0.8549166666666667 avg loss: 0.4166715216797882\n",
      "epoch 10  validation accuracy score: 0.8429 avg validation loss: 0.4423015387682982\n",
      "epoch 11  accuracy score: 0.857 avg loss: 0.4094242151863169\n",
      "epoch 11  validation accuracy score: 0.8452 avg validation loss: 0.43656359964755803\n",
      "epoch 12  accuracy score: 0.8590833333333333 avg loss: 0.4029689469895643\n",
      "epoch 12  validation accuracy score: 0.8472 avg validation loss: 0.43158889421379937\n",
      "epoch 13  accuracy score: 0.8603833333333334 avg loss: 0.39704979698880055\n",
      "epoch 13  validation accuracy score: 0.8488 avg validation loss: 0.42730622646136834\n",
      "epoch 14  accuracy score: 0.8618833333333333 avg loss: 0.3916121861634731\n",
      "epoch 14  validation accuracy score: 0.8507 avg validation loss: 0.42330312731252817\n",
      "epoch 15  accuracy score: 0.86405 avg loss: 0.38658850816122753\n",
      "epoch 15  validation accuracy score: 0.8519 avg validation loss: 0.4197313480346863\n",
      "epoch 16  accuracy score: 0.8657 avg loss: 0.38191338398106983\n",
      "epoch 16  validation accuracy score: 0.8533 avg validation loss: 0.4165254195913551\n",
      "epoch 17  accuracy score: 0.8669833333333333 avg loss: 0.3775308716650248\n",
      "epoch 17  validation accuracy score: 0.8553 avg validation loss: 0.4135576602032729\n",
      "epoch 18  accuracy score: 0.8682 avg loss: 0.3734660145733716\n",
      "epoch 18  validation accuracy score: 0.8569 avg validation loss: 0.41060739289960985\n",
      "epoch 19  accuracy score: 0.86995 avg loss: 0.36958210798591334\n",
      "epoch 19  validation accuracy score: 0.8565 avg validation loss: 0.40793237452603864\n",
      "epoch 20  accuracy score: 0.87095 avg loss: 0.3659231193109696\n",
      "epoch 20  validation accuracy score: 0.8574 avg validation loss: 0.4054346183475569\n"
     ]
    }
   ],
   "source": [
    "model.fit(trainx=train_x,trainy=train_y,validx=test_x,validy=test_y,epochs=20,batchsize=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
